# JAXified_ML | Numpy on steroids
## Implementation of major ML Architecture and techniques using JAX and Flax 

### Summary
This repository contains the implementation of major ML architectures and techniques using JAX and Flax. The goal is to provide a simple and easy to understand implementation of the architectures and techniques. The code is written in a modular way so that it can be easily extended and modified.

Jax is supposed to be faster than PyTorch and Tensorflow for specific tasks. Hoping to see some speedup in the training time and inference time without any other optimization.

## Project is under development.

### Done 
 * Kmeans
 * LeNet-5
 * ResNet

### TODO:
 * AlexNet
 * VGG
 * GoogLeNet
 * ResNetXt
 * RCNN
 * YOLO
 * SegNet
 * GAN
 * Transformer
    * BERT  

### Citation
```bibtex
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
} 
```